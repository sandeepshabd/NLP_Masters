{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "18452M8MGYCOaGn-PD5A1GnHtodf-R53d",
      "authorship_tag": "ABX9TyObzvJaJJIG9dp/NHOe85tl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepshabd/NLP_Masters/blob/master/week1/token_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxQZgmMw2u3S",
        "outputId": "5fd23367-aa0a-42bf-df9e-04a090997112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# tokenizer.py\n",
        "\n",
        "import re\n",
        "# ðŸ‘‡ï¸ for Anaconda\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# ðŸ‘‡ï¸ for Anaconda\n",
        "#conda install -c conda-forge spacy\n",
        "#python -m spacy download en_core_web_sm\n",
        "#python -m spacy download en\n",
        "\n",
        "# Tokenizes a string. Takes a string (a sentence), splits out punctuation and contractions, and returns a list of\n",
        "# strings, with each string being a token.\n",
        "\n",
        "def readFileAndToeknize():\n",
        "    wordCounter = {}\n",
        "\n",
        "    string = readFile()\n",
        "    stringArray = word_tokenize(string)\n",
        "    for word in stringArray:\n",
        "        if(word.isalpha()):\n",
        "            if(word in  wordCounter):\n",
        "                wordCounter[word] += 1\n",
        "            else:\n",
        "                wordCounter[word] = 1\n",
        "\n",
        "    #sortedByCount = sorted(wordCounter.items(),key=lambda x:x[1],)\n",
        "    sortedByCount = sorted(wordCounter.items(),key=lambda x:x[1],reverse=True)\n",
        "    countList = []\n",
        "    wordList = []\n",
        "    out_arr = []\n",
        "\n",
        "\n",
        "    for i in range(0, len(sortedByCount)):\n",
        "      countList.append(sortedByCount[i][1])\n",
        "      wordList.append(sortedByCount[i][0])\n",
        "      if(sortedByCount[i][1] != 0):\n",
        "        #scaling the graph\n",
        "        out_arr.append(1000/sortedByCount[i][1])\n",
        "      else:\n",
        "        out_arr.append(np.NAN)\n",
        "\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      ax.scatter(countList, out_arr)\n",
        "\n",
        "    for i, txt in enumerate(wordList):\n",
        "        ax.annotate(txt, (countList[i], out_arr[i]))\n",
        "\n",
        "    #print(sortedByCount[:10])\n",
        "\n",
        "\"\"\"     for key in  wordCounter:\n",
        "        if(wordCounter[key] >5):\n",
        "            print(key+\":\"+str(wordCounter[key]))\n",
        "     \"\"\"\n",
        "\n",
        "\n",
        "def readFile():\n",
        "    f = open(\"/content/sample_data/nyt.txt\", \"r\")\n",
        "    string = f.read()\n",
        "    f.close()\n",
        "    return string\n",
        "\n",
        "def tokenize(string):\n",
        "    # print(repr(string))\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'`\\-\\\"]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\"\\.\", \" . \", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\(\", \" ( \", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\-\", \" - \", string)\n",
        "    string = re.sub(r\"\\\"\", \" \\\" \", string)\n",
        "    # We may have introduced double spaces, so collapse these down\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return list(filter(lambda x: len(x) > 0, string.split(\" \")))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "   readFileAndToeknize()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "o3848gJo4AHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Oiu1m7Mv23q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "0LmfEwHC28sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "0gNZcUhN29UZ"
      }
    }
  ]
}
